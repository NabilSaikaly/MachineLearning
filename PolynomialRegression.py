# When the data between the independent variable and dependent variable is shaped in a way
# that is not linear, YET if the function/relationship  can be modelled as polynomial function,
# (i.e.) we can represent it in the form of (a_n)(x^n) + (a_n-1)(x^n-1) + ... + (a_1)(x^1) + (a_0)
# we can use the Polynomial function and transform/express it to/as a multiple linear regression
# And by that we would use the multiple linear regression training/testing procedures in order
# to solve a polynomial regression problem.
# Transformation is in the following:
# Suppose the Polynomial is P(x) = (a3)(x^3) + (a2)(x^2) + (a1)(x) + (a4)
# we pose x1 = x;  x2 = x^2 and x3=x^3
# => P(x) = (a3)x3 + (a2)x2 + (a1)x1 + a4
# We can now say that x1, x2 and x3 are FEATURES and a1,a2,a3 are PARAMETERS to a 
# MULTIPLE linear regression. x1,x2,x3 are the INDEPENDENT VARIABLES! 
# and P(x) = y is the Dependent variable that we are predicting.
# Now we will need to model the feature we have (x) so that it will have values of:
# x, x^2 and x^3, i.e the new variables.
# From the same data set that we modelled a Simple and Multiple Linear Regression, we will be 
# model this data set based on a polynomial regression model. It may be more accurate!
#Sometimes, the trend of data is not really linear, and looks curvy. 
#In this case we can use Polynomial regression methods.
# To know when to use a linear model or a non-linear model, plot each independent variable
# vs. the dependent variable and check the linearity visually.
# Then, determine the correlation coefficient between each independent and dependent variable,
# if it is >=0.7 for all cases=> There is a linear tendency and more appropriate to use Linear.
# If Non-Linear regression is to be used, you can represent it as a polynomial regression, then
# transform it to a multiple linear regression.
# Or, you can simply execute the non-linear regression functions and classes in sklearn/linear_model

import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt
from sklearn import linear_model 
from sklearn.preprocessing import PolynomialFeatures 
from sklearn.metrics import r2_score

df = pd.read_csv('FuelConsumptionCo2.csv')


#Taking Look at the dataset and forming a new dataframe that suits our needs.
# print(df.head())

cdf = df[['ENGINESIZE','CYLINDERS','FUELCONSUMPTION_COMB','CO2EMISSIONS']]


# Plotting the EngineSize (Independent Feature, aka Exploratory Feature)
# vs. Emission (Target)

plt.scatter(cdf.ENGINESIZE, cdf.CO2EMISSIONS, color ='blue')
plt.xlabel("Engine Size")
plt.ylabel("Emissions")
plt.show()


# Initializing the Train and Test data set, based on the Train/Test Splitting. 
# (Out-Of-Sample Testing) 20% for Testing and 80% for Training

msk = np.random.rand(len(df)) < 0.8
train = cdf[msk]
test = cdf[~msk]

train_x = np.asanyarray(train[['ENGINESIZE']])
train_y = np.asanyarray(train[['CO2EMISSIONS']])

test_x = np.asanyarray(train[['ENGINESIZE']])
test_y = np.asanyarray(train[['CO2EMISSIONS']])


#Transforming the non-linear regression, to a polynomial function, to a multiple linear reg.
#Here the polynomial is of degree 2. 
#We are going to create x1 and x2 from the feature set train_x (Engine Size)
#PolynomialFeatures() creates a schema of new features set from a single feature set!
# .fit_transform(EngineSizeSet) will produce EngineSize^0, EngineSize^1 and EngineSize^2
poly = PolynomialFeatures(degree=2)
train_x_poly = poly.fit_transform(train_x)

#Now we transformed the problem into a multiple linear regression!
regr = linear_model.LinearRegression()
regr.fit(train_x_poly, train_y)
print("\tDegree 2 Polynomial: ")
print("Coefficients/Parameters: ",regr.coef_)
print("Intercept: ", regr.intercept_)

# Visualizing the trend-line that we generated by transforming 
# a polynomial regression to a multiple linear regression.

plt.scatter(train_x, train_y, color ='blue')
trendline_xdata = np.arange(1.0,10.0,0.1)
trendline_ydata = regr.coef_[0][1]*trendline_xdata + regr.coef_[0][2]*np.power(trendline_xdata,2) + regr.intercept_
plt.plot(trendline_xdata, trendline_ydata, '-r')
plt.xlabel("Engine size")
plt.ylabel("Emission")
plt.show()


# Testing the Model we created with an out-of-sample testing set.

test_x_poly = poly.transform(test_x)
test_y_ = regr.predict(test_x_poly)
print("Mean absolute error: ", np.mean(np.absolute(test_y_ - test_y)))
print("Residual sum of squares (MSE): ", np.mean((test_y_ - test_y) ** 2))
print("R2-score: ", r2_score(test_y,test_y_ ) )


#using a polynomial regression with the dataset but this time with degree three (cubic).
# to check for accuracy!
poly = PolynomialFeatures(degree=3)
train_x_poly = poly.fit_transform(train_x)
regr = linear_model.LinearRegression()
regr.fit(train_x_poly, train_y)
test_x_poly = poly.transform(test_x)
test_y_ = regr.predict(test_x_poly)

print("\n\tDegree 3 Polynomial:")
print('Coefficients: ', regr.coef_)
print('Intercepts: ', regr.intercept_)
plt.scatter(train.ENGINESIZE, train.CO2EMISSIONS, color = 'blue')
XX = np.arange(0.0, 10.0, 0.1)
YY = regr.intercept_[0] + regr.coef_[0][1]*XX + regr.coef_[0][2]*np.power(XX,2) + regr.coef_[0][3]*np.power(XX, 3)
plt.plot(XX, YY, '-r')
plt.xlabel('Engine Size')
plt.ylabel('CO2 Emissions')
print("Mean absolute error: ", np.mean(np.absolute(test_y_ - test_y)))
print("Residual sum of squares (MSE): " , np.mean((test_y_ - test_y) ** 2))
print("R2-score: " , r2_score(test_y,test_y_ ) )
